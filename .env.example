# =============================================================================
# AI Assistant — Environment Configuration
# Copy this file to .env and fill in your values.
# =============================================================================

# -----------------------------------------------------------------------------
# Required
# -----------------------------------------------------------------------------
ANTHROPIC_API_KEY=sk-ant-...

# -----------------------------------------------------------------------------
# Core
# -----------------------------------------------------------------------------
ASSISTANT_MODEL=claude-sonnet-4-20250514
ASSISTANT_MAX_TOKENS=8192
ASSISTANT_DATA_DIR=./data
# DEBUG | INFO | WARNING | ERROR
ASSISTANT_LOG_LEVEL=INFO

# Server (for `assistant serve`)
ASSISTANT_HOST=127.0.0.1
ASSISTANT_PORT=51430

# Session
ASSISTANT_MAX_CONTEXT_MESSAGES=50
# auto-approve read-only tool calls
ASSISTANT_AUTO_APPROVE_READS=true
# max tool-use rounds per LLM call before forcing a summary
ASSISTANT_MAX_TOOL_ROUNDS=10

# Default provider — which LLM to use (claude | openai | gemini | ollama)
ASSISTANT_DEFAULT_PROVIDER=claude

# -----------------------------------------------------------------------------
# OpenAI
# -----------------------------------------------------------------------------
OPENAI_API_KEY=sk-...
ASSISTANT_OPENAI_ENABLED=false
ASSISTANT_OPENAI_MODEL=gpt-4o

# -----------------------------------------------------------------------------
# Google Gemini — uses OpenAI-compatible endpoint
# -----------------------------------------------------------------------------
GEMINI_API_KEY=
ASSISTANT_GEMINI_ENABLED=false
ASSISTANT_GEMINI_MODEL=gemini-2.0-flash

# -----------------------------------------------------------------------------
# Ollama — local LLM routing
# Simple/short requests are routed to the local model to save API costs.
# -----------------------------------------------------------------------------
ASSISTANT_OLLAMA_ENABLED=false
ASSISTANT_OLLAMA_BASE_URL=http://host.docker.internal:11434
ASSISTANT_OLLAMA_MODEL=llama3.2
# requests under this go to Ollama
ASSISTANT_LOCAL_MODEL_THRESHOLD_TOKENS=1000

# -----------------------------------------------------------------------------
# Web Search — SearXNG integration
# Self-host SearXNG: https://docs.searxng.org/
# -----------------------------------------------------------------------------
ASSISTANT_WEB_SEARCH_ENABLED=false
ASSISTANT_SEARXNG_URL=http://searxng:8080

# -----------------------------------------------------------------------------
# Vector Search — semantic similarity over conversation history
# Requires Ollama to be running with an embedding model available.
# -----------------------------------------------------------------------------
ASSISTANT_VECTOR_SEARCH_ENABLED=false
ASSISTANT_EMBEDDING_MODEL=nomic-embed-text

# -----------------------------------------------------------------------------
# Media
# -----------------------------------------------------------------------------
ASSISTANT_MAX_MEDIA_SIZE_MB=10

# -----------------------------------------------------------------------------
# Telegram bot
# Create a bot via @BotFather and paste the token below.
# TELEGRAM_ALLOWED_USER_IDS is a comma-separated list of numeric user IDs.
# -----------------------------------------------------------------------------
ASSISTANT_TELEGRAM_ENABLED=false
TELEGRAM_BOT_TOKEN=
# e.g. 123456789,987654321
ASSISTANT_TELEGRAM_ALLOWED_USER_IDS=
# leave blank to use long-polling
ASSISTANT_TELEGRAM_WEBHOOK_URL=

# -----------------------------------------------------------------------------
# WhatsApp bot — neonize linked-device pairing (scan QR on first run)
# WHATSAPP_ALLOWED_NUMBERS is a comma-separated list of E.164 numbers.
# -----------------------------------------------------------------------------
ASSISTANT_WHATSAPP_ENABLED=false
# e.g. +15551234567,+442071234567
ASSISTANT_WHATSAPP_ALLOWED_NUMBERS=
ASSISTANT_WHATSAPP_SESSION_NAME=assistant

# -----------------------------------------------------------------------------
# Scheduler — APScheduler-based automations
# -----------------------------------------------------------------------------
ASSISTANT_SCHEDULER_ENABLED=false
# e.g. America/New_York, Europe/London
ASSISTANT_SCHEDULER_TIMEZONE=UTC

# -----------------------------------------------------------------------------
# Plugins — third-party plugin system
# -----------------------------------------------------------------------------
ASSISTANT_PLUGINS_ENABLED=false

# -----------------------------------------------------------------------------
# Agents — LLM-driven sub-agents (researcher, coder, etc.)
# Define agents as YAML files in data/agents/.
# -----------------------------------------------------------------------------
ASSISTANT_AGENTS_ENABLED=false

# -----------------------------------------------------------------------------
# Watchdog — file-system monitoring
# WATCHDOG_PATHS is a comma-separated list of paths to watch.
# -----------------------------------------------------------------------------
ASSISTANT_WATCHDOG_ENABLED=false
# e.g. /home/user/docs,/tmp/incoming
ASSISTANT_WATCHDOG_PATHS=
ASSISTANT_WATCHDOG_DEBOUNCE_SECONDS=2.0
# auto-analyze changed files with LLM
ASSISTANT_WATCHDOG_ANALYZE=false

# -----------------------------------------------------------------------------
# Webhooks — inbound HTTP triggers mapped to workflows
# Sign payloads with the secret to pass HMAC-SHA256 validation.
# -----------------------------------------------------------------------------
ASSISTANT_WEBHOOK_ENABLED=false
# generate with: openssl rand -hex 32
ASSISTANT_WEBHOOK_SECRET=

# -----------------------------------------------------------------------------
# Do-Not-Disturb — queue non-urgent notifications during quiet hours
# Times are in 24-hour HH:MM format, interpreted in SCHEDULER_TIMEZONE.
# -----------------------------------------------------------------------------
ASSISTANT_DND_ENABLED=false
ASSISTANT_DND_START=23:00
ASSISTANT_DND_END=07:00
# urgent messages always get through
ASSISTANT_DND_ALLOW_URGENT=true
